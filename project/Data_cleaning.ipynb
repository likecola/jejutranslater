{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('your.csv', encoding='utf-8')\n",
    "\n",
    "# 데이터 확인 (예시: 상위 5개 행 출력)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터프레임에서 원문(사투리)과 번역(표준어) 열을 추출하여 리스트에 저장\n",
    "Original_sentences = data['Original'].tolist()\n",
    "translation_sentences = data['Translation'].tolist()\n",
    "\n",
    "# 리스트에 저장된 문장 확인 (예시: 상위 5개 문장 출력)\n",
    "print(Original_sentences[:5])\n",
    "print(translation_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 10개 문장 쌍 출력\n",
    "for Original, translation in zip(Original_sentences[:10], translation_sentences[:10]):\n",
    "    print(f'[사투리]: {Original}')\n",
    "    print(f'[표준어]: {translation}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사투리 토크나이저 훈련 데이터 제작\n",
    "with open('train_Original.txt', 'w', encoding='utf-8') as f:\n",
    "    for Original_sentence in Original_sentences:\n",
    "        f.write(Original_sentence + '\\n')\n",
    "\n",
    "# 표준어 토크나이저 훈련 데이터 제작\n",
    "with open('train_translation.txt', 'w', encoding='utf-8') as f:\n",
    "    for translation_sentence in translation_sentences:\n",
    "        f.write(translation_sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okt로 모든 데이터 토큰화 진행\n",
    "### 원문과 번역문 병렬 구조쌍 만들기 (+토큰화)\n",
    "\n",
    "토큰화와 병렬 문장 쌍 생성의 순서는 중요합니다. 일반적으로 다음과 같은 순서로 작업을 진행합니다:\n",
    "\n",
    "원문과 번역문 데이터 로드: 먼저 원문과 번역문 데이터를 로드합니다.\n",
    "\n",
    "토큰화: 원문 및 번역문을 토큰화하여 각각의 토큰 목록을 생성합니다. 이때, 토큰화는 언어 모델 또는 형태소 분석기를 사용하여 수행할 수 있습니다.\n",
    "\n",
    "병렬 문장 쌍 생성: 원문과 번역문의 토큰 목록을 이용하여 병렬 문장 쌍을 생성합니다. 쌍은 원문 문장과 해당 번역문 문장으로 구성됩니다.\n",
    "\n",
    "순서를 지키지 않으면 토큰화를 하기 전에 데이터가 무작위로 섞일 수 있으며, 이로 인해 원문과 번역문 간의 대응 관계가 깨질 수 있습니다. 따라서 일반적으로 위의 순서를 따르는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 데이터 디렉토리 경로\n",
    "base_directory = '/content/drive/MyDrive/jeju/'\n",
    "\n",
    "# CSV 파일 경로 설정 (원문 및 번역 모두 저장)\n",
    "csv_file = '/content/drive/MyDrive/jeju/parallel_data.csv'\n",
    "\n",
    "# CSV 파일 헤더 설정\n",
    "csv_columns = ['Utterance', 'Original', 'Translation']\n",
    "\n",
    "# 데이터 저장 리스트\n",
    "parallel_data = []\n",
    "\n",
    "# 모든 데이터 폴더 선택 (Training 및 Validation)\n",
    "data_directories = [\n",
    "    os.path.join(base_directory, 'Training'),\n",
    "    os.path.join(base_directory, 'Validation')\n",
    "]\n",
    "\n",
    "# 원문 및 번역 데이터를 읽어와서 병렬 구조 쌍으로 만들기\n",
    "for data_directory in data_directories:\n",
    "    data_files = [os.path.join(root, file) for root, dirs, files in os.walk(data_directory) for file in files if file.endswith('.json')]\n",
    "\n",
    "    for data_file in data_files:\n",
    "        with open(data_file, 'r', encoding='utf-8') as file:\n",
    "            data_json = json.load(file)\n",
    "            for idx, utterance in enumerate(data_json['utterance']):\n",
    "                original_text = utterance['dialect_form']\n",
    "                translation_text = utterance['standard_form']  # 번역 데이터로 사용할 열 선택\n",
    "\n",
    "                # 토큰화 수행\n",
    "                original_tokens = okt.morphs(original_text)\n",
    "                translation_tokens = okt.morphs(translation_text)\n",
    "\n",
    "                parallel_data.append({\n",
    "                    'Utterance': f'Utterance_{idx + 1}',\n",
    "                    'Original': ' '.join(original_tokens),  # 토큰을 공백으로 구분하여 저장\n",
    "                    'Translation': ' '.join(translation_tokens)  # 토큰을 공백으로 구분하여 저장\n",
    "                })\n",
    "\n",
    "# CSV 파일에 병렬 데이터 저장\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "    writer.writeheader()  # 헤더 쓰기\n",
    "\n",
    "    for parallel_item in parallel_data:\n",
    "        writer.writerow({\n",
    "            'Utterance': parallel_item['Utterance'],\n",
    "            'Original': parallel_item['Original'],\n",
    "            'Translation': parallel_item['Translation']\n",
    "        })\n",
    "\n",
    "print(f\"병렬 데이터가 {csv_file} 경로에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰을 추가하는 후처리 로직\n",
    "각 언어 쌍의 평균 길이와 최대 길이를 계산하고,\n",
    "[PAD] 토큰을 추가하여 모든 문장을 최대 길이로 맞추는 작업을 수행합니다.\n",
    "\n",
    "### 모든 데이터셋을 텐서형 데이터로 변환\n",
    "- 전처리와 후처리를 모두 마친 데이터들을 torch.Tensor로 변환해줍니다.\n",
    "- 변환 후, DataLoader를 활용해 데이터들을 배치로 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# CSV 파일 경로 설정\n",
    "csv_file = '/content/drive/MyDrive/jeju/parallel_data.csv'\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "parallel_data = []\n",
    "\n",
    "# CSV 파일에서 모든 데이터 읽어오기\n",
    "with open(csv_file, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        parallel_data.append({\n",
    "            'Utterance': row['Utterance'],\n",
    "            'Original': row['Original'],\n",
    "            'Translation': row['Translation']\n",
    "        })\n",
    "\n",
    "# 추가 후처리 작업 (SOS, EOS, PAD 토큰 추가)\n",
    "max_len = 32  # 문장의 최대 길이를 지정합니다.\n",
    "for item in parallel_data:\n",
    "    item['Original'] = '[SOS] ' + item['Original'] + ' [EOS]'\n",
    "    item['Translation'] = '[SOS] ' + item['Translation'] + ' [EOS]'\n",
    "\n",
    "# 단어를 정수로 매핑하는 사전을 생성합니다.\n",
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "# 특수 토큰을 사전에 추가합니다.\n",
    "special_tokens = ['[PAD]', '[SOS]', '[EOS]']\n",
    "for token in special_tokens:\n",
    "    word_to_index[token] = len(word_to_index)\n",
    "    index_to_word[len(word_to_index) - 1] = token\n",
    "\n",
    "# 토큰화 및 후처리 함수\n",
    "def tokenize_and_preprocess(sentence, max_len):\n",
    "    tokens = okt.morphs(sentence)[:max_len-2]  # 최대 길이를 고려하여 자르기\n",
    "    tokens = ['[SOS]'] + tokens + ['[EOS]']\n",
    "    tokens += ['[PAD]'] * (max_len - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "# 데이터를 토큰화 및 후처리하여 텐서로 변환하는 함수\n",
    "def process_data(data, max_len):\n",
    "    tokenized_data = [tokenize_and_preprocess(item['Original'], max_len) for item in tqdm(data, desc=\"Tokenizing Data\")]  # tqdm 추가\n",
    "    tensor_data = [torch.tensor(tokens_to_indices(tokens, max_len)).long() for tokens in tqdm(tokenized_data, desc=\"Converting to Tensors\")]  # tqdm 추가\n",
    "    return tensor_data\n",
    "\n",
    "# 토큰을 정수 인덱스로 매핑하는 함수\n",
    "def tokens_to_indices(tokens, max_len):\n",
    "    indices = [word_to_index.get(token, word_to_index['[PAD]']) for token in tokens]\n",
    "    indices += [word_to_index['[PAD]']] * (max_len - len(indices))\n",
    "    return indices\n",
    "\n",
    "# 문장의 최대 길이를 계산합니다.\n",
    "jeju_lengths = [len(tokenize_and_preprocess(item['Original'], max_len)) for item in parallel_data]\n",
    "std_lengths = [len(tokenize_and_preprocess(item['Translation'], max_len)) for item in parallel_data]\n",
    "\n",
    "# 제주도 사투리 데이터의 평균 및 최대 길이 계산\n",
    "average_jeju_length = sum(jeju_lengths) / len(jeju_lengths)\n",
    "max_jeju_length = max(jeju_lengths)\n",
    "\n",
    "# 표준어 데이터의 평균 및 최대 길이 계산\n",
    "average_std_length = sum(std_lengths) / len(std_lengths)\n",
    "max_std_length = max(std_lengths)\n",
    "\n",
    "# 수정할 문장의 최대 길이를 선택\n",
    "# 여기서는 각각의 데이터 세트에 대한 최대 길이 중 더 큰 값을 선택합니다.\n",
    "max_len = max(max_jeju_length, max_std_length)\n",
    "\n",
    "# 데이터를 토큰화 및 후처리하여 텐서로 변환\n",
    "jeju_tensors = process_data(parallel_data, max_len)\n",
    "std_tensors = process_data(parallel_data, max_len)\n",
    "\n",
    "# DataLoader를 사용하여 데이터를 배치로 만듭니다.\n",
    "batch_size = 32  # 배치 크기 조정 가능\n",
    "jeju_loader = DataLoader(TensorDataset(*jeju_tensors), batch_size=batch_size, shuffle=True)\n",
    "std_loader = DataLoader(TensorDataset(*std_tensors), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 데이터 사이즈 확인\n",
    "jeju_data_size = len(jeju_loader.dataset)\n",
    "std_data_size = len(std_loader.dataset)\n",
    "\n",
    "print(f\"제주도 사투리 데이터의 전체 크기: {jeju_data_size} 문장\")\n",
    "print(f\"표준어 데이터의 전체 크기: {std_data_size} 문장\")\n",
    "\n",
    "# 배치 수 계산\n",
    "jeju_batches = len(jeju_loader)\n",
    "std_batches = len(std_loader)\n",
    "\n",
    "print(f\"제주도 사투리 데이터의 배치 수: {jeju_batches} 배치\")\n",
    "print(f\"표준어 데이터의 배치 수: {std_batches} 배치\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# 처리된 데이터 파일 경로\n",
    "processed_data_file = '/content/drive/MyDrive/jeju/parallel_data.csv'\n",
    "\n",
    "# 데이터를 저장할 리스트\n",
    "processed_data = []\n",
    "\n",
    "# CSV 파일에서 데이터 읽어오기\n",
    "with open(processed_data_file, 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        processed_data.append({\n",
    "            'Utterance': row['Utterance'],\n",
    "            'Original': row['Original'],\n",
    "            'Translation': row['Translation']\n",
    "        })\n",
    "\n",
    "# 후처리 결과 확인을 위한 예시 문장 출력\n",
    "num_examples = 5  # 출력할 예시 문장 개수\n",
    "for i, item in enumerate(processed_data[:num_examples]):\n",
    "    print(f\"Example {i + 1}:\")\n",
    "    print(f\"Original: {item['Original']}\")\n",
    "    print(f\"Translation: {item['Translation']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# Okt 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 스페셜 토큰 정의\n",
    "special_tokens = ['[PAD]', '[SOS]', '[EOS]']\n",
    "\n",
    "# 스페셜 토큰에 대한 인덱스 매핑 딕셔너리 생성\n",
    "special_token_to_index = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "\n",
    "# 스페셜 토큰에 대한 인덱스를 얻을 수 있습니다.\n",
    "pad_idx = special_token_to_index.get('[PAD]')\n",
    "sos_idx = special_token_to_index.get('[SOS]')\n",
    "eos_idx = special_token_to_index.get('[EOS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 문장 (처리한 데이터 중 하나를 선택)\n",
    "sample_translation = parallel_data[51055]['Translation']\n",
    "\n",
    "# 토큰화\n",
    "sample_tokens = sample_translation.split()\n",
    "\n",
    "# 후처리 작업 결과 (SOS, EOS, PAD 토큰 제외)\n",
    "post_proc_sent = [word_to_index.get(token, word_to_index['[PAD]']) for token in sample_tokens if token not in ['[SOS]', '[EOS]', '[PAD]']]\n",
    "\n",
    "print(f'후처리 결과: {post_proc_sent}\\n')\n",
    "print(f'후처리 해석: {\" \".join(sample_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(parallel_data)\n",
    "print(f\"데이터셋의 크기: {dataset_size}\")\n",
    "\n",
    "# 데이터셋 크기가 283,929인 경우, 이 데이터셋에는 총 283,929개의 데이터 포인트가 있으며,\n",
    "# 각 데이터 포인트에는 여러 개의 토큰이 아닌 원시 텍스트 형태로 문장 또는 문장 쌍이 포함되어 있을 것입니다.\n",
    "# 따라서 토큰 수를 세려면 각 데이터 포인트를 토큰화한 다음 모든 토큰의 수를 합산한 것임."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
